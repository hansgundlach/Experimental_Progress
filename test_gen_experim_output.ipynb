{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test gen_experim Output\n",
        "\n",
        "Test script to print gen_experim output for multiple model sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the function\n",
        "from experiment_utils import (\n",
        "    gen_experim,\n",
        "    calculate_transformer_params,\n",
        "    calculate_non_embedding_params,\n",
        "    get_base_config,\n",
        ")\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_experiment(hidden_dim, label_suffix=\"\", activation=\"swiglu\"):\n",
        "    \"\"\"Analyze a single experiment configuration\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT ANALYSIS: {hidden_dim}d Model{label_suffix}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Generate the experiment\n",
        "    result = gen_experim(\n",
        "        hidden_dim,\n",
        "        label=f\"{hidden_dim}d_test_experiment\",\n",
        "        learning_rate=0.001,\n",
        "        activation=activation,\n",
        "    )\n",
        "\n",
        "    # Extract the configuration\n",
        "    exp_group = result[0]\n",
        "    sub_exp = exp_group[\"subexperiments\"][0]\n",
        "    config = sub_exp[\"overrides\"]\n",
        "\n",
        "    print(f\"Experiment Group Name: {exp_group['name']}\")\n",
        "    print(f\"Label: {sub_exp['label']}\")\n",
        "    print(\"\\nGenerated Configuration Parameters:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Print each parameter nicely formatted\n",
        "    for key, value in sorted(config.items()):\n",
        "        if isinstance(value, (int, float)) and value > 1000:\n",
        "            print(f\"{key:25}: {value:,}\")\n",
        "        else:\n",
        "            print(f\"{key:25}: {value}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"Key Scaling Information:\")\n",
        "\n",
        "    # Calculate the number of parameters (with weight tying by default).\n",
        "    # Use the activation from the generated config so SwiGLU vs GELU\n",
        "    # get the correct parameter counts.\n",
        "    act = config.get(\"activation\", \"swiglu\")\n",
        "    num_params = calculate_transformer_params(\n",
        "        config[\"hidden_dim\"],\n",
        "        config[\"num_layers\"],\n",
        "        activation=act,\n",
        "        tie_embeddings=True,\n",
        "    )\n",
        "\n",
        "    # Calculate non-embedding parameters (Kaplan et al. definition)\n",
        "    non_embedding_params = calculate_non_embedding_params(\n",
        "        config[\"hidden_dim\"],\n",
        "        config[\"num_layers\"],\n",
        "        activation=act,\n",
        "        tie_embeddings=True,\n",
        "    )\n",
        "\n",
        "    print(f\"- Hidden dimension: {config['hidden_dim']}\")\n",
        "    print(f\"- Number of layers: {config['num_layers']} (scaled proportionally)\")\n",
        "    print(f\"- Number of heads: {config['num_heads']}\")\n",
        "    print(f\"- Head dimension: {config['hidden_dim'] // config['num_heads']}\")\n",
        "    print(f\"- Total parameters: {num_params:,}\")\n",
        "    print(\n",
        "        f\"- Non-embedding parameters: {non_embedding_params:,} (Kaplan et al. definition)\"\n",
        "    )\n",
        "    print(\n",
        "        f\"- Token limit: {config['max_tokens_training']:,} tokens (20x parameters = {20 * num_params:,})\"\n",
        "    )\n",
        "    print(f\"- Gradient accumulation: {config['gradient_accumulation_steps']} steps\")\n",
        "\n",
        "    # Calculate batch size breakdown\n",
        "    base_config = get_base_config()\n",
        "    target_effective_batch_size = base_config[\"target_effective_batch_size\"]\n",
        "    per_step_batch_size = config[\"batch_size\"]  # Calculated per-step batch size\n",
        "    grad_accum = config[\"gradient_accumulation_steps\"]\n",
        "    effective_batch_size = per_step_batch_size * grad_accum\n",
        "\n",
        "    print(\n",
        "        f\"- Target effective batch size: {target_effective_batch_size} (optimization goal)\"\n",
        "    )\n",
        "    print(\n",
        "        f\"- Per-step batch size: {per_step_batch_size} (calculated to fit in GPU memory)\"\n",
        "    )\n",
        "    print(f\"- Gradient accumulation steps: {grad_accum}\")\n",
        "    print(f\"- Effective batch size: {effective_batch_size} (per_step × grad_accum)\")\n",
        "    print(f\"- Learning rate: {config['learning_rate']} (your override)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SWIGLU PARAMETERS\n",
        "\n",
        "Test different hidden dimensions to see how the experiment configurations scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing gen_experim function output for multiple model sizes...\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 32d Model (1/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 32d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 32\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 33,824,980\n",
            "num_heads                : 2\n",
            "num_layers               : 2\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 32\n",
            "- Number of layers: 2 (scaled proportionally)\n",
            "- Number of heads: 2\n",
            "- Head dimension: 16\n",
            "- Total parameters: 1,691,249\n",
            "- Non-embedding parameters: 83,025 (Kaplan et al. definition)\n",
            "- Token limit: 33,824,980 tokens (20x parameters = 33,824,980)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 48d Model (2/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 48d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 48\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 51,463,700\n",
            "num_heads                : 3\n",
            "num_layers               : 3\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 48\n",
            "- Number of layers: 3 (scaled proportionally)\n",
            "- Number of heads: 3\n",
            "- Head dimension: 16\n",
            "- Total parameters: 2,573,185\n",
            "- Non-embedding parameters: 160,849 (Kaplan et al. definition)\n",
            "- Token limit: 51,463,700 tokens (20x parameters = 51,463,700)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 64d Model (3/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 64d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 64\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 70,576,980\n",
            "num_heads                : 4\n",
            "num_layers               : 4\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 64\n",
            "- Number of layers: 4 (scaled proportionally)\n",
            "- Number of heads: 4\n",
            "- Head dimension: 16\n",
            "- Total parameters: 3,528,849\n",
            "- Non-embedding parameters: 312,401 (Kaplan et al. definition)\n",
            "- Token limit: 70,576,980 tokens (20x parameters = 70,576,980)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 96d Model (4/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 96d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 96\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 115,193,300\n",
            "num_heads                : 6\n",
            "num_layers               : 6\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 96\n",
            "- Number of layers: 6 (scaled proportionally)\n",
            "- Number of heads: 6\n",
            "- Head dimension: 16\n",
            "- Total parameters: 5,759,665\n",
            "- Non-embedding parameters: 934,993 (Kaplan et al. definition)\n",
            "- Token limit: 115,193,300 tokens (20x parameters = 115,193,300)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 128d Model (5/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 128d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 128\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 171,606,100\n",
            "num_heads                : 8\n",
            "num_layers               : 8\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 128\n",
            "- Number of layers: 8 (scaled proportionally)\n",
            "- Number of heads: 8\n",
            "- Head dimension: 16\n",
            "- Total parameters: 8,580,305\n",
            "- Non-embedding parameters: 2,147,409 (Kaplan et al. definition)\n",
            "- Token limit: 171,606,100 tokens (20x parameters = 171,606,100)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 160d Model (6/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 160d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 160\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 243,747,540\n",
            "num_heads                : 10\n",
            "num_layers               : 10\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 160\n",
            "- Number of layers: 10 (scaled proportionally)\n",
            "- Number of heads: 10\n",
            "- Head dimension: 16\n",
            "- Total parameters: 12,187,377\n",
            "- Non-embedding parameters: 4,146,257 (Kaplan et al. definition)\n",
            "- Token limit: 243,747,540 tokens (20x parameters = 243,747,540)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 256d Model (7/7)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMORY DIAGNOSTIC FOR 256d MODEL:\n",
            "  Total params: 29,693,265\n",
            "  Num layers: 16, Num heads: 16\n",
            "  GPU type: V100\n",
            "  Target effective batch size: 64\n",
            "  Calculated grad_accum_steps: 2\n",
            "  Per-step batch size: 32\n",
            "  Sequence length: 128\n",
            "  Max training tokens: 593,865,300\n",
            "============================================================\n",
            "\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 256d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : swiglu\n",
            "batch_size               : 32\n",
            "gradient_accumulation_steps: 2\n",
            "hidden_dim               : 256\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 593,865,300\n",
            "num_heads                : 16\n",
            "num_layers               : 16\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 256\n",
            "- Number of layers: 16 (scaled proportionally)\n",
            "- Number of heads: 16\n",
            "- Head dimension: 16\n",
            "- Total parameters: 29,693,265\n",
            "- Non-embedding parameters: 16,827,473 (Kaplan et al. definition)\n",
            "- Token limit: 593,865,300 tokens (20x parameters = 593,865,300)\n",
            "- Gradient accumulation: 2 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 32 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 2\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "SUMMARY: All experiments analyzed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing gen_experim function output for multiple model sizes...\")\n",
        "\n",
        "# Test different hidden dimensions\n",
        "hidden_dims = [32, 48, 64, 96, 128, 160, 256]\n",
        "\n",
        "for i, hidden_dim in enumerate(hidden_dims):\n",
        "    analyze_experiment(hidden_dim, f\" ({i+1}/{len(hidden_dims)})\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY: All experiments analyzed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GELU Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing gen_experim function output for multiple model sizes...\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 32d Model (1/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 32d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 32\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 33,661,140\n",
            "num_heads                : 2\n",
            "num_layers               : 2\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 32\n",
            "- Number of layers: 2 (scaled proportionally)\n",
            "- Number of heads: 2\n",
            "- Head dimension: 16\n",
            "- Total parameters: 1,683,057\n",
            "- Non-embedding parameters: 74,833 (Kaplan et al. definition)\n",
            "- Token limit: 33,661,140 tokens (20x parameters = 33,661,140)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 48d Model (2/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 48d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 48\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 50,910,740\n",
            "num_heads                : 3\n",
            "num_layers               : 3\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 48\n",
            "- Number of layers: 3 (scaled proportionally)\n",
            "- Number of heads: 3\n",
            "- Head dimension: 16\n",
            "- Total parameters: 2,545,537\n",
            "- Non-embedding parameters: 133,201 (Kaplan et al. definition)\n",
            "- Token limit: 50,910,740 tokens (20x parameters = 50,910,740)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 64d Model (3/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 64d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 64\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 69,266,260\n",
            "num_heads                : 4\n",
            "num_layers               : 4\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 64\n",
            "- Number of layers: 4 (scaled proportionally)\n",
            "- Number of heads: 4\n",
            "- Head dimension: 16\n",
            "- Total parameters: 3,463,313\n",
            "- Non-embedding parameters: 246,865 (Kaplan et al. definition)\n",
            "- Token limit: 69,266,260 tokens (20x parameters = 69,266,260)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 96d Model (4/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 96d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 96\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 110,769,620\n",
            "num_heads                : 6\n",
            "num_layers               : 6\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 96\n",
            "- Number of layers: 6 (scaled proportionally)\n",
            "- Number of heads: 6\n",
            "- Head dimension: 16\n",
            "- Total parameters: 5,538,481\n",
            "- Non-embedding parameters: 713,809 (Kaplan et al. definition)\n",
            "- Token limit: 110,769,620 tokens (20x parameters = 110,769,620)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 128d Model (5/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 128d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 128\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 161,120,340\n",
            "num_heads                : 8\n",
            "num_layers               : 8\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 128\n",
            "- Number of layers: 8 (scaled proportionally)\n",
            "- Number of heads: 8\n",
            "- Head dimension: 16\n",
            "- Total parameters: 8,056,017\n",
            "- Non-embedding parameters: 1,623,121 (Kaplan et al. definition)\n",
            "- Token limit: 161,120,340 tokens (20x parameters = 161,120,340)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 160d Model (6/7)\n",
            "============================================================\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 160d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 64\n",
            "gradient_accumulation_steps: 1\n",
            "hidden_dim               : 160\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 223,267,540\n",
            "num_heads                : 10\n",
            "num_layers               : 10\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 160\n",
            "- Number of layers: 10 (scaled proportionally)\n",
            "- Number of heads: 10\n",
            "- Head dimension: 16\n",
            "- Total parameters: 11,163,377\n",
            "- Non-embedding parameters: 3,122,257 (Kaplan et al. definition)\n",
            "- Token limit: 223,267,540 tokens (20x parameters = 223,267,540)\n",
            "- Gradient accumulation: 1 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 64 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 1\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT ANALYSIS: 256d Model (7/7)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMORY DIAGNOSTIC FOR 256d MODEL:\n",
            "  Total params: 25,498,961\n",
            "  Num layers: 16, Num heads: 16\n",
            "  GPU type: V100\n",
            "  Target effective batch size: 64\n",
            "  Calculated grad_accum_steps: 2\n",
            "  Per-step batch size: 32\n",
            "  Sequence length: 128\n",
            "  Max training tokens: 509,979,220\n",
            "============================================================\n",
            "\n",
            "Experiment Group Name: generated_experiments_v100\n",
            "Label: 256d_test_experiment\n",
            "\n",
            "Generated Configuration Parameters:\n",
            "----------------------------------------\n",
            "activation               : gelu\n",
            "batch_size               : 32\n",
            "gradient_accumulation_steps: 2\n",
            "hidden_dim               : 256\n",
            "learning_rate            : 0.001\n",
            "max_tokens_training      : 509,979,220\n",
            "num_heads                : 16\n",
            "num_layers               : 16\n",
            "\n",
            "----------------------------------------\n",
            "Key Scaling Information:\n",
            "- Hidden dimension: 256\n",
            "- Number of layers: 16 (scaled proportionally)\n",
            "- Number of heads: 16\n",
            "- Head dimension: 16\n",
            "- Total parameters: 25,498,961\n",
            "- Non-embedding parameters: 12,633,169 (Kaplan et al. definition)\n",
            "- Token limit: 509,979,220 tokens (20x parameters = 509,979,220)\n",
            "- Gradient accumulation: 2 steps\n",
            "- Target effective batch size: 64 (optimization goal)\n",
            "- Per-step batch size: 32 (calculated to fit in GPU memory)\n",
            "- Gradient accumulation steps: 2\n",
            "- Effective batch size: 64 (per_step × grad_accum)\n",
            "- Learning rate: 0.001 (your override)\n",
            "\n",
            "============================================================\n",
            "SUMMARY: All experiments analyzed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing gen_experim function output for multiple model sizes...\")\n",
        "\n",
        "# Test different hidden dimensions\n",
        "hidden_dims = [32, 48, 64, 96, 128, 160, 256]\n",
        "\n",
        "for i, hidden_dim in enumerate(hidden_dims):\n",
        "    analyze_experiment(hidden_dim, f\" ({i+1}/{len(hidden_dims)})\", activation=\"gelu\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY: All experiments analyzed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "othello",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
