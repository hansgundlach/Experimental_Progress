
#add pre-LN setup 


# glu and swiglu
#implement learning rate schedules in term of steps
#download flash attention in bash file 
#remove config parameters that no longer do anything
#control for randomness across runs to get better replicability and cross model comparison 