
# get flash attention to work on models 
# test model on 8 gpu setup 
# update pytorch to get nn.RMSnorm
# figure out why torch.compile is failing and fix it. 









#add pre-LN setup 
# glu and swiglu
#implement learning rate schedules in term of steps
#download flash attention in bash file 
#remove config parameters that no longer do anything
#control for randomness across runs to get better replicability and cross model comparison 
#divid up main_experiment.py to make more readable
