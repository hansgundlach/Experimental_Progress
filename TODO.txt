

#correct issues with learning rate experiments 
# add straight to zero 
# read about parameter settings ie depth to width material 









# be able to run multiple GPUs with LSTM 
#figure out how to properly scale LSTM
#set up scaling experiments with LSTM 
#find sources for how to scale transformer 











# TODO be able to sampel with a given random seed from LSTM 




#
run again with c4
be able to run chinchilla level experiment complexity 
be able to run parallelized version of llstm 
be able to do experiments with different lstm configs 






# get flash attention to work on models 
# test model on 8 gpu setup 
# update pytorch to get nn.RMSnorm
# figure out why torch.compile is failing and fix it. 









#add pre-LN setup 
# glu and swiglu
#implement learning rate schedules in term of steps
#download flash attention in bash file 
#remove config parameters that no longer do anything
#control for randomness across runs to get better replicability and cross model comparison 
#divid up main_experiment.py to make more readable
