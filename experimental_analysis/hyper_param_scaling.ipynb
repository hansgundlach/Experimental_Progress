{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset': 'c4_subset', 'batch_size': 1, 'learning_rate': 0.001, 'min_lr': 1e-05, 'lr_schedule': 'cosine', 'warmup_epochs': 1, 'warmup_epochs_frac': 0.1, 'weight_decay': 0.1, 'hidden_dim': 256, 'num_layers': 16, 'num_heads': 16, 'dropout': 0.0, 'seq_length': 128, 'wikitext_limit': 50000000, 'pos_encoding': 'rotary', 'init_scheme': 'transformer_scaled', 'stride': 64, 'pin_memory': True, 'compile': False, 'prefetch_factor': 8, 'min_epochs': 2, 'max_epochs': 218454, 'use_gradient_clipping': True, 'gradient_clip_val': 2.0, 'label_smoothing': 0.0, 'gradient_accumulation_steps': 16, 'optimizer': 'adamw', 'activation': 'gelu', 'norm_type': 'layer', 'results_folder': 'Former_Experiments_Folder', 'csv_log_interval': 50, 'seed': 789, 'target_tokens': 251658240}]\n"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "\n",
    "base_config = {\n",
    "        \"dataset\": \"c4_subset\",\n",
    "        \"batch_size\": 32,  # physical batch size 256\n",
    "        \"learning_rate\": 0.001 * math.sqrt(4),\n",
    "        \"min_lr\": 1e-5,\n",
    "        \"lr_schedule\": \"cosine\",\n",
    "        \"warmup_epochs\": 1,\n",
    "        \"warmup_epochs_frac\": 0.1,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"hidden_dim\": 64,  # Base hidden dimension\n",
    "        \"num_layers\": 4,  # Base number of layers\n",
    "        \"num_heads\": 4,\n",
    "        \"dropout\": 0.0,\n",
    "        \"seq_length\": 128,\n",
    "        \"wikitext_limit\": 5 * 10**7,\n",
    "        \"pos_encoding\": \"rotary\",\n",
    "        \"init_scheme\": \"transformer_scaled\",\n",
    "        \"stride\": 64,\n",
    "        \"pin_memory\": True,\n",
    "        \"compile\": False,\n",
    "        \"prefetch_factor\": 8,\n",
    "        \"min_epochs\": 2,\n",
    "        \"max_epochs\": 2,\n",
    "        \"use_gradient_clipping\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"label_smoothing\": 0.0,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm_type\": \"layer\",\n",
    "        \"results_folder\": \"Former_Experiments_Folder\",\n",
    "        \"csv_log_interval\": 50,\n",
    "        \"seed\": 789,\n",
    "    }\n",
    "\n",
    "def chinchilla_scale(base_cfg, hidden_dims):\n",
    "    \"\"\"\n",
    "    Return a list of configs that satisfy:\n",
    "      • tokens ≈ 20 × parameters\n",
    "      • per-step compute budget unchanged vs. baseline\n",
    "      • depth/width ratio fixed (layers ∝ hidden_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def param_count(d, L):\n",
    "        # crude but width-dominant: 12·L·d²  (ignores embeddings/out-proj)\n",
    "        return 12 * L * d**2\n",
    "\n",
    "    base_d = base_cfg[\"hidden_dim\"]\n",
    "    base_L = base_cfg[\"num_layers\"]\n",
    "    base_bsz = base_cfg[\"batch_size\"]\n",
    "    base_lr = base_cfg[\"learning_rate\"]\n",
    "    base_clip = base_cfg[\"gradient_clip_val\"]\n",
    "    seq_len = base_cfg[\"seq_length\"]\n",
    "\n",
    "    out = []\n",
    "    for d in hidden_dims:\n",
    "        width_scale = d / base_d\n",
    "\n",
    "        # 1) Depth: keep L ∝ d   (so aspect-ratio is preserved)\n",
    "        L = max(1, int(round(base_L * width_scale)))\n",
    "\n",
    "        # 2) Keep per-step FLOPs ≈ const ⇒ batch ∝ 1 / (width² · depth/base_depth)\n",
    "        flops_scale = (width_scale**2) * (L / base_L)\n",
    "        bsz = max(1, int(round(base_bsz / flops_scale)))\n",
    "\n",
    "        # 3) LR & grad-clip heuristics\n",
    "        lr = base_lr * (base_d / d) ** 0.5\n",
    "        clip = base_clip * math.sqrt(width_scale)\n",
    "\n",
    "        # 4) Chinchilla target tokens  (≈ 20 × parameters)\n",
    "        params = param_count(d, L)\n",
    "        tgt_tok = int(20 * params)\n",
    "\n",
    "        # 5) Convert token target into epochs\n",
    "        tokens_per_step = bsz * seq_len\n",
    "        est_steps = math.ceil(tgt_tok / tokens_per_step)\n",
    "        max_epochs = math.ceil(\n",
    "            est_steps / (len(base_cfg.get(\"dataset\", [])) or 1)\n",
    "        )  # adjust as needed\n",
    "\n",
    "        cfg = copy.deepcopy(base_cfg)\n",
    "        cfg.update(\n",
    "            {\n",
    "                \"hidden_dim\": d,\n",
    "                \"num_layers\": L,\n",
    "                \"num_heads\": max(1, d // 16),\n",
    "                \"batch_size\": bsz,\n",
    "                \"learning_rate\": lr,\n",
    "                \"gradient_clip_val\": clip,\n",
    "                \"target_tokens\": tgt_tok,\n",
    "                \"max_epochs\": max(max_epochs, cfg.get(\"min_epochs\", 1)),\n",
    "            }\n",
    "        )\n",
    "        out.append(cfg)\n",
    "    return out\n",
    "\n",
    "\n",
    "print(chinchilla_scale(base_config, [256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overrides:\n",
      "{'hidden_dim': 16, 'num_layers': 2, 'num_heads': 1, 'learning_rate': 0.001, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 16205120}\n",
      "810256 for  16 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 24, 'num_layers': 3, 'num_heads': 1, 'learning_rate': 0.001224744871391589, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 24538080}\n",
      "1226904 for  24 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 32, 'num_layers': 3, 'num_heads': 2, 'learning_rate': 0.0014142135623730952, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 32901760}\n",
      "1645088 for  32 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 48, 'num_layers': 4, 'num_heads': 3, 'learning_rate': 0.0017320508075688772, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 50458560}\n",
      "2522928 for  48 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 64, 'num_layers': 4, 'num_heads': 4, 'learning_rate': 0.002, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 68261120}\n",
      "3413056 for  64 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 96, 'num_layers': 6, 'num_heads': 6, 'learning_rate': 0.002449489742783178, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 109764480}\n",
      "5488224 for  96 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 128, 'num_layers': 8, 'num_heads': 8, 'learning_rate': 0.0028284271247461905, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 160115200}\n",
      "8005760 for  128 d model\n",
      "{'dim16': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 16,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 1,\n",
      "           'learning_rate': 0.001,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 16205120},\n",
      " 'dim24': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 24,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 1,\n",
      "           'learning_rate': 0.001224744871391589,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 24538080},\n",
      " 'dim32': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 32,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.0014142135623730952,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 32901760},\n",
      " 'dim48': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 48,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 3,\n",
      "           'learning_rate': 0.0017320508075688772,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 50458560},\n",
      " 'dim64': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 64,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 4,\n",
      "           'learning_rate': 0.002,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 68261120},\n",
      " 'dim96': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 96,\n",
      "           'num_layers': 6,\n",
      "           'num_heads': 6,\n",
      "           'learning_rate': 0.002449489742783178,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 109764480},\n",
      " 'dim128': {'dataset': 'c4_subset',\n",
      "            'lr_schedule': 'cosine',\n",
      "            'warmup_epochs': 1,\n",
      "            'warmup_epochs_frac': 0.1,\n",
      "            'weight_decay': 0.1,\n",
      "            'dropout': 0.0,\n",
      "            'seq_length': 128,\n",
      "            'pos_encoding': 'rotary',\n",
      "            'init_scheme': 'transformer_scaled',\n",
      "            'stride': 64,\n",
      "            'pin_memory': True,\n",
      "            'compile': False,\n",
      "            'prefetch_factor': 8,\n",
      "            'min_epochs': 1,\n",
      "            'max_epochs': 1,\n",
      "            'use_gradient_clipping': True,\n",
      "            'gradient_clip_val': 1.0,\n",
      "            'label_smoothing': 0.0,\n",
      "            'optimizer': 'adamw',\n",
      "            'activation': 'gelu',\n",
      "            'norm_type': 'layer',\n",
      "            'results_folder': 'Former_Experiments_Folder',\n",
      "            'csv_log_interval': 50,\n",
      "            'seed': 789,\n",
      "            'hidden_dim': 128,\n",
      "            'num_layers': 8,\n",
      "            'num_heads': 8,\n",
      "            'learning_rate': 0.0028284271247461905,\n",
      "            'batch_size': 32,\n",
      "            'gradient_accumulation_steps': 8,\n",
      "            'train_tokens': 160115200}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# ---------- common settings ----------\n",
    "COMMON = dict(\n",
    "    dataset             = \"c4_subset\",\n",
    "    lr_schedule         = \"cosine\",\n",
    "    warmup_epochs       = 1,\n",
    "    warmup_epochs_frac  = 0.10,\n",
    "    weight_decay        = 0.10,\n",
    "    dropout             = 0.0,          # bump to 0.1-0.2 for >100 M tokens if needed\n",
    "    seq_length          = 128,\n",
    "    pos_encoding        = \"rotary\",\n",
    "    init_scheme         = \"transformer_scaled\",\n",
    "    stride              = 64,\n",
    "    pin_memory          = True,\n",
    "    compile             = False,\n",
    "    prefetch_factor     = 8,\n",
    "    min_epochs          = 1,\n",
    "    max_epochs          = 1,\n",
    "    use_gradient_clipping = True,\n",
    "    gradient_clip_val   = 1.0,\n",
    "    label_smoothing     = 0.0,\n",
    "    optimizer           = \"adamw\",\n",
    "    activation          = \"gelu\",\n",
    "    norm_type           = \"layer\",\n",
    "    results_folder      = \"Former_Experiments_Folder\",\n",
    "    csv_log_interval    = 50,\n",
    "    seed                = 789,\n",
    ")\n",
    "\n",
    "GPT2_VOCAB_SIZE = 50257\n",
    "\n",
    "def make_cfg(d_model, n_layers, vocab:int = GPT2_VOCAB_SIZE):\n",
    "    heads = max(1, d_model // 16)\n",
    "    lr    = 0.001 * math.sqrt(d_model / 16)\n",
    "    params = 12 * d_model * d_model * n_layers + vocab * d_model    # rough GPT-style count\n",
    "    tokens = 20 * params                           # Chin. optimal compute\n",
    "    eff_bs = 256                                   # keep effective batch ~constant\n",
    "    phys_bs = 32     \n",
    "    \n",
    "    print(\"Overrides:\")     \n",
    "    print(dict(\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    ))        \n",
    "    print(params, \"for \", d_model, \"d model\")                 # fits typical 40 GB A100 w/ acc-16\n",
    "\n",
    "    return dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    )\n",
    "\n",
    "\n",
    "#choose layer depth so that it is roughly proportional to the hidden dimension cubed\n",
    "CONFIGS = {\n",
    "    \"dim16\"  : make_cfg(16,  2),\n",
    "    \"dim24\"  : make_cfg(24,  3),\n",
    "    \"dim32\"  : make_cfg(32,  3),\n",
    "    \"dim48\"  : make_cfg(48,  4),\n",
    "    \"dim64\"  : make_cfg(64,  4),   # original width/depth\n",
    "    \"dim96\"  : make_cfg(96,  6),\n",
    "    \"dim128\" : make_cfg(128, 8),\n",
    "}\n",
    "\n",
    "# Pretty-print if you run this file directly\n",
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "    pprint(CONFIGS, width=120, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Scaling Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810256 this is 16 size\n",
      "1219992 this is 24 size\n",
      "1632800 this is 32 size\n",
      "2495280 this is 48 size\n",
      "2927288 this is 56 size\n",
      "3413056 this is 64 size\n",
      "3867336 this is 72 size\n",
      "4404560 this is 80 size\n",
      "5488224 this is 96 size\n",
      "8005760 this is 128 size\n",
      "{'dim16': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 16,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.044721359549995794,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 16205120},\n",
      " 'dim24': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 24,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.03651483716701107,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 24399840},\n",
      " 'dim32': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 32,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.03162277660168379,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 32656000},\n",
      " 'dim48': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 48,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.025819888974716113,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 49905600},\n",
      " 'dim56': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 56,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.023904572186687872,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 58545760},\n",
      " 'dim64': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 64,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.022360679774997897,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 68261120},\n",
      " 'dim72': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 72,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.021081851067789193,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 77346720},\n",
      " 'dim80': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 80,\n",
      "           'num_layers': 5,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.02,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 88091200},\n",
      " 'dim96': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 96,\n",
      "           'num_layers': 6,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.018257418583505537,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 109764480},\n",
      " 'dim128': {'dataset': 'c4_subset',\n",
      "            'lr_schedule': 'cosine',\n",
      "            'warmup_epochs': 1,\n",
      "            'warmup_epochs_frac': 0.1,\n",
      "            'weight_decay': 0.1,\n",
      "            'dropout': 0.0,\n",
      "            'seq_length': 128,\n",
      "            'pos_encoding': 'rotary',\n",
      "            'init_scheme': 'transformer_scaled',\n",
      "            'stride': 64,\n",
      "            'pin_memory': True,\n",
      "            'compile': False,\n",
      "            'prefetch_factor': 8,\n",
      "            'min_epochs': 1,\n",
      "            'max_epochs': 1,\n",
      "            'use_gradient_clipping': True,\n",
      "            'gradient_clip_val': 1.0,\n",
      "            'label_smoothing': 0.0,\n",
      "            'optimizer': 'adamw',\n",
      "            'activation': 'gelu',\n",
      "            'norm_type': 'layer',\n",
      "            'results_folder': 'Former_Experiments_Folder',\n",
      "            'csv_log_interval': 50,\n",
      "            'seed': 789,\n",
      "            'hidden_dim': 128,\n",
      "            'num_layers': 8,\n",
      "            'num_heads': 2,\n",
      "            'learning_rate': 0.015811388300841896,\n",
      "            'batch_size': 32,\n",
      "            'gradient_accumulation_steps': 8,\n",
      "            'train_tokens': 160115200}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# === Literature-backed heuristics ==========================================\n",
    "\n",
    "HEAD_DIM = 64\n",
    "# Vaswani et al., 2017 §3.2: “we set d_k = d_v = d_model / h = 64 for each head.”\n",
    "# https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "# BASE_LR = 6e-4\n",
    "#new rate base on sgd sweep \n",
    "BASE_LR = 10**(-1.5)\n",
    "\n",
    "\n",
    "# Yang et al., 2022 (Tensor Programs V) §4: “optimal LR scales ∝ width^{-½}.”\n",
    "# https://arxiv.org/pdf/2203.03466.pdf\n",
    "\n",
    "CHIN_TOK_RATIO = 20\n",
    "# Hoffmann et al., 2022 (Chinchilla) §3: “compute-optimal: ~20 tokens per parameter.”\n",
    "# https://arxiv.org/pdf/2203.15556.pdf\n",
    "\n",
    "EFF_BS  = 256                      # keep effective batch fixed\n",
    "PHYS_BS = 32                       # Goyal et al., 2017: LR ∝ batch (linear-scaling rule)\n",
    "# https://arxiv.org/pdf/1706.02677.pdf\n",
    "\n",
    "GPT2_VOCAB_SIZE = 50_257\n",
    "\n",
    "# === Scaling rules =========================================================\n",
    "\n",
    "def make_cfg(d_model: int,\n",
    "             vocab: int = GPT2_VOCAB_SIZE):\n",
    "    # Depth grows ≈ 0.06 × width (TinyStories sweep, Eldan & Li 2023, Fig. 5)\n",
    "    # https://arxiv.org/pdf/2305.07759.pdf\n",
    "    n_layers = max(2, round(0.06 * d_model))\n",
    "\n",
    "    # 2–8 heads so each head keeps ≥64-d and avoids the low-rank bottleneck\n",
    "    # Bhojanapalli et al., 2020: “excess heads create a low-rank bottleneck.”\n",
    "    # https://proceedings.mlr.press/v119/bhojanapalli20a/bhojanapalli20a.pdf\n",
    "    # Saratchandran et al., 2025 (Leaner Transformers): “more heads lets you cut depth.”\n",
    "    # https://arxiv.org/pdf/2505.20802.pdf\n",
    "    n_heads  = max(2, min(8, round(d_model / HEAD_DIM)))\n",
    "\n",
    "    # Width-scaled LR (μP theory) + 2025 plateau exponent ≈ 0.22 (Li et al., 2025)\n",
    "    # https://arxiv.org/pdf/2503.04715.pdf\n",
    "    lr_base_dim = 32\n",
    "    lr = BASE_LR * (d_model / lr_base_dim) ** (-0.5)\n",
    "\n",
    "    # Rough GPT-style parameter count (Kaplan et al., 2020 formula)\n",
    "    params = 12 * d_model * d_model * n_layers + vocab * d_model\n",
    "    print(params, f\"this is {d_model} size\")\n",
    "\n",
    "    # Compute-optimal token budget (Chinchilla)\n",
    "    tokens = CHIN_TOK_RATIO * params\n",
    "\n",
    "    return dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = n_heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = PHYS_BS,\n",
    "        gradient_accumulation_steps = EFF_BS // PHYS_BS,\n",
    "        train_tokens = tokens,\n",
    "    )\n",
    "\n",
    "# --- Config grid -----------------------------------------------------------\n",
    "\n",
    "CONFIGS = {\n",
    "    f\"dim{d}\": make_cfg(d) for d in [16, 24, 32, 48, 56, 64, 72, 80, 96, 128]\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "    pprint(CONFIGS, width=100, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete-P Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_params_count\u001b[39m(shape: ModelShape) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Rough GPT‑style param count (same as before)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m12\u001b[39m \u001b[38;5;241m*\u001b[39m shape\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m*\u001b[39m shape\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m*\u001b[39m shape\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m+\u001b[39m shape\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m*\u001b[39m shape\u001b[38;5;241m.\u001b[39md_model\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_cfg_completep\u001b[39m(\n\u001b[1;32m     67\u001b[0m     d_model: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     68\u001b[0m     vocab: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m GPT2_VOCAB_SIZE,\n\u001b[0;32m---> 69\u001b[0m     shape: \u001b[43mModelShape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m     alpha: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,  \u001b[38;5;66;03m# Complete‑P\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Returns a config dict with Complete‑P scaling baked in.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Iterable, List\n",
    "import math\n",
    "\n",
    "# ---------- common settings ----------\n",
    "COMMON = dict(\n",
    "    dataset             = \"c4_subset\",\n",
    "    lr_schedule         = \"cosine\",\n",
    "    warmup_epochs       = 1,\n",
    "    warmup_epochs_frac  = 0.10,\n",
    "    weight_decay_base   = 0.10,   # λ_base; actual λ scales with mN\n",
    "    dropout             = 0.0,\n",
    "    seq_length          = 128,\n",
    "    pos_encoding        = \"rotary\",\n",
    "    init_scheme         = \"transformer_scaled\",\n",
    "    stride              = 64,\n",
    "    pin_memory          = True,\n",
    "    compile             = False,\n",
    "    prefetch_factor     = 8,\n",
    "    min_epochs          = 1,\n",
    "    max_epochs          = 1,\n",
    "    use_gradient_clipping = True,\n",
    "    gradient_clip_val   = 1.0,\n",
    "    label_smoothing     = 0.0,\n",
    "    optimizer           = \"adamw\",\n",
    "    activation          = \"gelu\",\n",
    "    norm_type           = \"layer\",\n",
    "    results_folder      = \"Former_Experiments_Folder\",\n",
    "    csv_log_interval    = 50,\n",
    "    seed                = 789,\n",
    ")\n",
    "\n",
    "HEAD_DIM = 64\n",
    "GPT2_VOCAB_SIZE = 50_257\n",
    "TPP = 20  # compute‑optimal default token budget ≈ 20 tokens/parameter (Chinchilla)\n",
    "\n",
    "# Base model used for m_N, m_L multipliers (Table 1 uses N_base=256, L_base=2):contentReference[oaicite:3]{index=3}\n",
    "N_BASE = 256\n",
    "L_BASE = 2\n",
    "\n",
    "# Use a base LR that you tune at the tiny base model; Complete‑P transfers across depth:contentReference[oaicite:4]{index=4}\n",
    "ETA_BASE = 3.9e-3  # from Fig. 2 base in paper; feel free to retune for your stack:contentReference[oaicite:5]{index=5}\n",
    "\n",
    "# AdamW epsilon base; width*depth scaling handled per‑group below (Appendix E.4):contentReference[oaicite:6]{index=6}\n",
    "EPS_BASE = 1e-16\n",
    "\n",
    "CHIN_TOK_RATIO = 20\n",
    "\n",
    "@dataclass\n",
    "class ModelShape:\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    vocab_size: int = GPT2_VOCAB_SIZE\n",
    "\n",
    "def _default_shape(d_model: int, vocab: int = GPT2_VOCAB_SIZE) -> ModelShape:\n",
    "    # Depth ≈ 0.06 × width (heuristic you already used)\n",
    "    n_layers = max(2, round(0.06 * d_model))\n",
    "    n_heads  = max(2, min(8, round(d_model / HEAD_DIM)))\n",
    "    return ModelShape(d_model=d_model, n_layers=n_layers, n_heads=n_heads, vocab_size=vocab)\n",
    "\n",
    "def _params_count(shape: ModelShape) -> int:\n",
    "    # Rough GPT‑style param count (same as before)\n",
    "    return 12 * shape.d_model * shape.d_model * shape.n_layers + shape.vocab_size * shape.d_model\n",
    "\n",
    "def make_cfg_completep(\n",
    "    d_model: int,\n",
    "    vocab: int = GPT2_VOCAB_SIZE,\n",
    "    shape: ModelShape | None = None,\n",
    "    alpha: float = 1.0,  # Complete‑P\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns a config dict with Complete‑P scaling baked in.\n",
    "    \"\"\"\n",
    "    if shape is None:\n",
    "        shape = _default_shape(d_model, vocab)\n",
    "\n",
    "    # width, depth multipliers (mN, mL) relative to base N=256, L=2:contentReference[oaicite:7]{index=7}\n",
    "    mN = shape.d_model / N_BASE\n",
    "    mL = shape.n_layers / L_BASE\n",
    "\n",
    "    # residual branch scaling factor L^{-α} (Eq. 1):contentReference[oaicite:8]{index=8}\n",
    "    residual_scale = mL ** (-alpha)\n",
    "\n",
    "    # Per‑group scalings from Table 1 with α=1:\n",
    "    # Hidden LR: η_base * mN^{-1} * mL^{α−1}  -> with α=1, becomes η_base * mN^{-1}\n",
    "    # Hidden WD: λ_base * mN\n",
    "    # PreLN LR:  η_base * mL^{α−1} -> no depth factor at α=1\n",
    "    # Bias LR (hidden): η_base * mL^{α−1} -> no depth factor at α=1\n",
    "    # AdamW ε (hidden): ε_base * mN^{-1} * mL^{-α} -> ε_base * mN^{-1} / mL\n",
    "    # Emb/Unemb LRs remain η_base; ε unchanged; unemb forward uses 1/mN (handled in model)\n",
    "    # LN gain/bias follow \"Pre‑LN LR\" & bias notes (Appendix E.2):contentReference[oaicite:9]{index=9}\n",
    "\n",
    "    # Global “headline” hyperparams (you can log these; real values are in param‑groups)\n",
    "    params = _params_count(shape)\n",
    "    train_tokens = max(int(CHIN_TOK_RATIO * params), 1)  # default 20 TPP\n",
    "\n",
    "    cfg = dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = shape.d_model,\n",
    "        num_layers  = shape.n_layers,\n",
    "        num_heads   = shape.n_heads,\n",
    "        vocab_size  = shape.vocab_size,\n",
    "        learning_rate_base = ETA_BASE,\n",
    "        epsilon_base = EPS_BASE,\n",
    "        weight_decay_base = COMMON[\"weight_decay_base\"],\n",
    "        residual_scale = residual_scale,\n",
    "        alpha = alpha,\n",
    "        n_base = N_BASE,\n",
    "        l_base = L_BASE,\n",
    "        mN = mN,\n",
    "        mL = mL,\n",
    "        batch_size  = 32,  # physical; adjust per hardware\n",
    "        gradient_accumulation_steps = 256 // 32,  # keep effective BS=256\n",
    "        train_tokens = train_tokens,\n",
    "        tpp = TPP,\n",
    "        completep_enabled = True,\n",
    "    )\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Scaling Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated LSTM Config for hidden_size=16 ---\n",
      "  Architecture: 2 layers, 16 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 1.61M\n",
      "  Data: Using 129.0M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=24 ---\n",
      "  Architecture: 2 layers, 24 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 2.42M\n",
      "  Data: Using 193.7M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=32 ---\n",
      "  Architecture: 2 layers, 32 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 3.23M\n",
      "  Data: Using 258.6M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=48 ---\n",
      "  Architecture: 2 layers, 48 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 4.86M\n",
      "  Data: Using 388.9M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=64 ---\n",
      "  Architecture: 2 layers, 64 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 6.50M\n",
      "  Data: Using 519.9M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=96 ---\n",
      "  Architecture: 2 layers, 96 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 9.80M\n",
      "  Data: Using 783.7M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=128 ---\n",
      "  Architecture: 2 layers, 128 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 13.13M\n",
      "  Data: Using 1050.2M characters from dataset\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"data_path\": \"../Datasets/wikitext.txt\",\n",
    "    \"tokenizer_path\": \"../gpt2_tokenizer\",\n",
    "    \"max_characters\": 5 * 1e7,  # Maximum number of characters to use from dataset\n",
    "    \"sequence_length\": 128,\n",
    "    \"batch_size\": 32,  # Keep physical batch size small, has no effect on model\n",
    "    \"hidden_size\": 16,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.0,  # dropout zer here to match transformer but may need to adjust for LSTM\n",
    "    \"learning_rate\": 0.001 * math.sqrt(4),  # Scale by sqrt of accumulation steps\n",
    "    \"lr_schedule\": \"cosine\",\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.1,  # parameter usedf for stepLR step decay\n",
    "    \"num_epochs\": 5,\n",
    "    \"train_split\": 0.8,\n",
    "    \"val_split\": 0.1,\n",
    "    \"test_split\": 0.1,\n",
    "    \"device\": \"cuda\",\n",
    "    \"wandb_project\": \"lstm-wikitext\",\n",
    "    \"wandb_offline\": True,\n",
    "    \"print_every\": 100,  # Print loss every N batches\n",
    "    # Gradient clipping settings\n",
    "    \"use_gradient_clipping\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    # NEW: CSV logging settings\n",
    "    \"results_folder\": \"Experiments_Folder\",\n",
    "    \"csv_log_interval\": 100,  # Log every 100 steps\n",
    "    # NEW: Data loading optimization settings\n",
    "    \"num_workers\": \"auto\",  # Will be set automatically based on CPU cores\n",
    "    \"pin_memory\": True,  # Faster GPU memory transfer\n",
    "    \"persistent_workers\": True,  # Keep data loading workers alive between epochs\n",
    "    \"prefetch_factor\": 4,  # Number of batches to prefetch per worker\n",
    "    # NEW: Mixed precision settings\n",
    "    \"use_amp\": False,  # Enable Automatic Mixed Precision\n",
    "    \"amp_opt_level\": \"O1\",  # Not used with native AMP, but kept for reference\n",
    "    # NEW: Gradient accumulation settings\n",
    "    \"gradient_accumulation_steps\": 16,  # For tracking only\n",
    "    # NEW: whether to compile the model (PyTorch 2.0+)\n",
    "    \"use_compile\": False,\n",
    "    \"seed\": 789,\n",
    "    \"optimizer\": \"adamw\",  # NEW: choose from \"adam\", \"adamw\", or \"sgd\"\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"stride\": 64,  # NEW: sliding-window stride to match transformer\n",
    "}\n",
    "\n",
    "\n",
    "def make_lstm_config(hidden_size, base_config=None):\n",
    "    \"\"\"\n",
    "    Generates a scaled LSTM configuration based on hidden dimension size.\n",
    "\n",
    "    This function applies heuristics and best practices for scaling LSTMs,\n",
    "    focusing on increasing width before depth and adjusting learning rate\n",
    "    and data size accordingly.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The target hidden dimension size for the LSTM.\n",
    "        base_config (dict, optional): A base configuration to build upon.\n",
    "                                      If None, uses sensible defaults from CONFIG.\n",
    "\n",
    "    Returns:\n",
    "        dict: A configuration dictionary scaled for the given hidden size.\n",
    "    \"\"\"\n",
    "    # 1. --- Set sensible base config if none provided ---\n",
    "    if base_config is None:\n",
    "        base_config = CONFIG\n",
    "    \n",
    "    config = base_config.copy()\n",
    "    config[\"hidden_size\"] = hidden_size\n",
    "\n",
    "    # 2. --- Scale LSTM architecture (Wider before Deeper) ---\n",
    "    # Depth (num_layers) should scale sub-linearly with width (hidden_size).\n",
    "    if hidden_size <= 128:\n",
    "        config[\"num_layers\"] = 2\n",
    "    elif hidden_size <= 512:\n",
    "        config[\"num_layers\"] = 3\n",
    "    else:\n",
    "        # It's rare for LSTMs to benefit from more than 4 layers\n",
    "        config[\"num_layers\"] = 4\n",
    "\n",
    "    # 3. --- Scale Learning Rate ---\n",
    "    # Use inverse square root scaling relative to a baseline.\n",
    "    base_lr = 1e-3\n",
    "    base_hidden = 128\n",
    "    # LR decreases as model size increases\n",
    "   \n",
    "    config['learning_rate'] = base_lr\n",
    "\n",
    "    # 4. --- Scale Dropout for Regularization ---\n",
    "    # Larger models can handle more dropout.\n",
    "    if hidden_size <= 256:\n",
    "        config[\"dropout\"] = 0.1\n",
    "    else:\n",
    "        config[\"dropout\"] = 0.2\n",
    "\n",
    "    # 5. --- Scale Data Amount (Chinchilla-style) ---\n",
    "    # First, estimate model parameters.\n",
    "    vocab_size = 50257  # GPT-2\n",
    "    # Params for one LSTM layer: 4 * (h*h (input-hidden) + h*h (hidden-hidden) + 2*h (biases))\n",
    "    # Simplified: 4 * (2*h^2) = 8*h^2\n",
    "    lstm_params = config[\"num_layers\"] * (8 * hidden_size**2)\n",
    "    embedding_params = vocab_size * hidden_size\n",
    "    output_params = hidden_size * vocab_size\n",
    "    total_params = lstm_params + embedding_params + output_params\n",
    "\n",
    "    # Chinchilla's rule: ~20 tokens per parameter\n",
    "    target_tokens = 20 * total_params\n",
    "    # Assume ~4 characters per token for English text\n",
    "    chars_per_token = 4\n",
    "    config[\"max_characters\"] = int(target_tokens * chars_per_token)\n",
    "\n",
    "    print(f\"\\n--- Generated LSTM Config for hidden_size={hidden_size} ---\")\n",
    "    print(f\"  Architecture: {config['num_layers']} layers, {config['hidden_size']} hidden_size\")\n",
    "    print(f\"  Training: LR={config['learning_rate']:.2e}, Dropout={config['dropout']:.2f}\")\n",
    "    print(f\"  Estimated Params: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"  Data: Using {config['max_characters'] / 1e6:.1f}M characters from dataset\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# Example of how to use the generator to create an experiment suite\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define the hidden sizes you want to test\n",
    "    lstm_hidden_sizes = [16, 24, 32, 48, 64, 96, 128]\n",
    "\n",
    "    # Create a dictionary of configuration objects\n",
    "    LSTM_EXPERIMENTS = {\n",
    "        f\"lstm_h{size}\": make_lstm_config(size) for size in lstm_hidden_sizes\n",
    "    }\n",
    "\n",
    "    # Now you can iterate through these configs to run your experiments\n",
    "    # For example:\n",
    "    # for exp_name, exp_config in LSTM_EXPERIMENTS.items():\n",
    "    #     print(f\"\\nRunning experiment: {exp_name}\")\n",
    "    #     # Here you would call your training function with exp_config\n",
    "    #     # train(config=exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
