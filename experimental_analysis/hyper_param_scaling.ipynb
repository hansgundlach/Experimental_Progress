{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset': 'c4_subset', 'batch_size': 1, 'learning_rate': 0.001, 'min_lr': 1e-05, 'lr_schedule': 'cosine', 'warmup_epochs': 1, 'warmup_epochs_frac': 0.1, 'weight_decay': 0.1, 'hidden_dim': 256, 'num_layers': 16, 'num_heads': 16, 'dropout': 0.0, 'seq_length': 128, 'wikitext_limit': 50000000, 'pos_encoding': 'rotary', 'init_scheme': 'transformer_scaled', 'stride': 64, 'pin_memory': True, 'compile': False, 'prefetch_factor': 8, 'min_epochs': 2, 'max_epochs': 218454, 'use_gradient_clipping': True, 'gradient_clip_val': 2.0, 'label_smoothing': 0.0, 'gradient_accumulation_steps': 16, 'optimizer': 'adamw', 'activation': 'gelu', 'norm_type': 'layer', 'results_folder': 'Former_Experiments_Folder', 'csv_log_interval': 50, 'seed': 789, 'target_tokens': 251658240}]\n"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "\n",
    "base_config = {\n",
    "        \"dataset\": \"c4_subset\",\n",
    "        \"batch_size\": 32,  # physical batch size 256\n",
    "        \"learning_rate\": 0.001 * math.sqrt(4),\n",
    "        \"min_lr\": 1e-5,\n",
    "        \"lr_schedule\": \"cosine\",\n",
    "        \"warmup_epochs\": 1,\n",
    "        \"warmup_epochs_frac\": 0.1,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"hidden_dim\": 64,  # Base hidden dimension\n",
    "        \"num_layers\": 4,  # Base number of layers\n",
    "        \"num_heads\": 4,\n",
    "        \"dropout\": 0.0,\n",
    "        \"seq_length\": 128,\n",
    "        \"wikitext_limit\": 5 * 10**7,\n",
    "        \"pos_encoding\": \"rotary\",\n",
    "        \"init_scheme\": \"transformer_scaled\",\n",
    "        \"stride\": 64,\n",
    "        \"pin_memory\": True,\n",
    "        \"compile\": False,\n",
    "        \"prefetch_factor\": 8,\n",
    "        \"min_epochs\": 2,\n",
    "        \"max_epochs\": 2,\n",
    "        \"use_gradient_clipping\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"label_smoothing\": 0.0,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm_type\": \"layer\",\n",
    "        \"results_folder\": \"Former_Experiments_Folder\",\n",
    "        \"csv_log_interval\": 50,\n",
    "        \"seed\": 789,\n",
    "    }\n",
    "\n",
    "def chinchilla_scale(base_cfg, hidden_dims):\n",
    "    \"\"\"\n",
    "    Return a list of configs that satisfy:\n",
    "      • tokens ≈ 20 × parameters\n",
    "      • per-step compute budget unchanged vs. baseline\n",
    "      • depth/width ratio fixed (layers ∝ hidden_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def param_count(d, L):\n",
    "        # crude but width-dominant: 12·L·d²  (ignores embeddings/out-proj)\n",
    "        return 12 * L * d**2\n",
    "\n",
    "    base_d = base_cfg[\"hidden_dim\"]\n",
    "    base_L = base_cfg[\"num_layers\"]\n",
    "    base_bsz = base_cfg[\"batch_size\"]\n",
    "    base_lr = base_cfg[\"learning_rate\"]\n",
    "    base_clip = base_cfg[\"gradient_clip_val\"]\n",
    "    seq_len = base_cfg[\"seq_length\"]\n",
    "\n",
    "    out = []\n",
    "    for d in hidden_dims:\n",
    "        width_scale = d / base_d\n",
    "\n",
    "        # 1) Depth: keep L ∝ d   (so aspect-ratio is preserved)\n",
    "        L = max(1, int(round(base_L * width_scale)))\n",
    "\n",
    "        # 2) Keep per-step FLOPs ≈ const ⇒ batch ∝ 1 / (width² · depth/base_depth)\n",
    "        flops_scale = (width_scale**2) * (L / base_L)\n",
    "        bsz = max(1, int(round(base_bsz / flops_scale)))\n",
    "\n",
    "        # 3) LR & grad-clip heuristics\n",
    "        lr = base_lr * (base_d / d) ** 0.5\n",
    "        clip = base_clip * math.sqrt(width_scale)\n",
    "\n",
    "        # 4) Chinchilla target tokens  (≈ 20 × parameters)\n",
    "        params = param_count(d, L)\n",
    "        tgt_tok = int(20 * params)\n",
    "\n",
    "        # 5) Convert token target into epochs\n",
    "        tokens_per_step = bsz * seq_len\n",
    "        est_steps = math.ceil(tgt_tok / tokens_per_step)\n",
    "        max_epochs = math.ceil(\n",
    "            est_steps / (len(base_cfg.get(\"dataset\", [])) or 1)\n",
    "        )  # adjust as needed\n",
    "\n",
    "        cfg = copy.deepcopy(base_cfg)\n",
    "        cfg.update(\n",
    "            {\n",
    "                \"hidden_dim\": d,\n",
    "                \"num_layers\": L,\n",
    "                \"num_heads\": max(1, d // 16),\n",
    "                \"batch_size\": bsz,\n",
    "                \"learning_rate\": lr,\n",
    "                \"gradient_clip_val\": clip,\n",
    "                \"target_tokens\": tgt_tok,\n",
    "                \"max_epochs\": max(max_epochs, cfg.get(\"min_epochs\", 1)),\n",
    "            }\n",
    "        )\n",
    "        out.append(cfg)\n",
    "    return out\n",
    "\n",
    "\n",
    "print(chinchilla_scale(base_config, [256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overrides:\n",
      "{'hidden_dim': 16, 'num_layers': 2, 'num_heads': 1, 'learning_rate': 0.001, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 16205120}\n",
      "810256 for  16 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 24, 'num_layers': 3, 'num_heads': 1, 'learning_rate': 0.001224744871391589, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 24538080}\n",
      "1226904 for  24 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 32, 'num_layers': 3, 'num_heads': 2, 'learning_rate': 0.0014142135623730952, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 32901760}\n",
      "1645088 for  32 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 48, 'num_layers': 4, 'num_heads': 3, 'learning_rate': 0.0017320508075688772, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 50458560}\n",
      "2522928 for  48 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 64, 'num_layers': 4, 'num_heads': 4, 'learning_rate': 0.002, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 68261120}\n",
      "3413056 for  64 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 96, 'num_layers': 6, 'num_heads': 6, 'learning_rate': 0.002449489742783178, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 109764480}\n",
      "5488224 for  96 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 128, 'num_layers': 8, 'num_heads': 8, 'learning_rate': 0.0028284271247461905, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 160115200}\n",
      "8005760 for  128 d model\n",
      "{'dim16': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 16,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 1,\n",
      "           'learning_rate': 0.001,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 16205120},\n",
      " 'dim24': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 24,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 1,\n",
      "           'learning_rate': 0.001224744871391589,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 24538080},\n",
      " 'dim32': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 32,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.0014142135623730952,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 32901760},\n",
      " 'dim48': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 48,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 3,\n",
      "           'learning_rate': 0.0017320508075688772,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 50458560},\n",
      " 'dim64': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 64,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 4,\n",
      "           'learning_rate': 0.002,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 68261120},\n",
      " 'dim96': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 96,\n",
      "           'num_layers': 6,\n",
      "           'num_heads': 6,\n",
      "           'learning_rate': 0.002449489742783178,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 109764480},\n",
      " 'dim128': {'dataset': 'c4_subset',\n",
      "            'lr_schedule': 'cosine',\n",
      "            'warmup_epochs': 1,\n",
      "            'warmup_epochs_frac': 0.1,\n",
      "            'weight_decay': 0.1,\n",
      "            'dropout': 0.0,\n",
      "            'seq_length': 128,\n",
      "            'pos_encoding': 'rotary',\n",
      "            'init_scheme': 'transformer_scaled',\n",
      "            'stride': 64,\n",
      "            'pin_memory': True,\n",
      "            'compile': False,\n",
      "            'prefetch_factor': 8,\n",
      "            'min_epochs': 1,\n",
      "            'max_epochs': 1,\n",
      "            'use_gradient_clipping': True,\n",
      "            'gradient_clip_val': 1.0,\n",
      "            'label_smoothing': 0.0,\n",
      "            'optimizer': 'adamw',\n",
      "            'activation': 'gelu',\n",
      "            'norm_type': 'layer',\n",
      "            'results_folder': 'Former_Experiments_Folder',\n",
      "            'csv_log_interval': 50,\n",
      "            'seed': 789,\n",
      "            'hidden_dim': 128,\n",
      "            'num_layers': 8,\n",
      "            'num_heads': 8,\n",
      "            'learning_rate': 0.0028284271247461905,\n",
      "            'batch_size': 32,\n",
      "            'gradient_accumulation_steps': 8,\n",
      "            'train_tokens': 160115200}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# ---------- common settings ----------\n",
    "COMMON = dict(\n",
    "    dataset             = \"c4_subset\",\n",
    "    lr_schedule         = \"cosine\",\n",
    "    warmup_epochs       = 1,\n",
    "    warmup_epochs_frac  = 0.10,\n",
    "    weight_decay        = 0.10,\n",
    "    dropout             = 0.0,          # bump to 0.1-0.2 for >100 M tokens if needed\n",
    "    seq_length          = 128,\n",
    "    pos_encoding        = \"rotary\",\n",
    "    init_scheme         = \"transformer_scaled\",\n",
    "    stride              = 64,\n",
    "    pin_memory          = True,\n",
    "    compile             = False,\n",
    "    prefetch_factor     = 8,\n",
    "    min_epochs          = 1,\n",
    "    max_epochs          = 1,\n",
    "    use_gradient_clipping = True,\n",
    "    gradient_clip_val   = 1.0,\n",
    "    label_smoothing     = 0.0,\n",
    "    optimizer           = \"adamw\",\n",
    "    activation          = \"gelu\",\n",
    "    norm_type           = \"layer\",\n",
    "    results_folder      = \"Former_Experiments_Folder\",\n",
    "    csv_log_interval    = 50,\n",
    "    seed                = 789,\n",
    ")\n",
    "\n",
    "GPT2_VOCAB_SIZE = 50257\n",
    "\n",
    "def make_cfg(d_model, n_layers, vocab:int = GPT2_VOCAB_SIZE):\n",
    "    heads = max(1, d_model // 16)\n",
    "    lr    = 0.001 * math.sqrt(d_model / 16)\n",
    "    params = 12 * d_model * d_model * n_layers + vocab * d_model    # rough GPT-style count\n",
    "    tokens = 20 * params                           # Chin. optimal compute\n",
    "    eff_bs = 256                                   # keep effective batch ~constant\n",
    "    phys_bs = 32     \n",
    "    \n",
    "    print(\"Overrides:\")     \n",
    "    print(dict(\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    ))        \n",
    "    print(params, \"for \", d_model, \"d model\")                 # fits typical 40 GB A100 w/ acc-16\n",
    "\n",
    "    return dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    )\n",
    "\n",
    "\n",
    "#choose layer depth so that it is roughly proportional to the hidden dimension cubed\n",
    "CONFIGS = {\n",
    "    \"dim16\"  : make_cfg(16,  2),\n",
    "    \"dim24\"  : make_cfg(24,  3),\n",
    "    \"dim32\"  : make_cfg(32,  3),\n",
    "    \"dim48\"  : make_cfg(48,  4),\n",
    "    \"dim64\"  : make_cfg(64,  4),   # original width/depth\n",
    "    \"dim96\"  : make_cfg(96,  6),\n",
    "    \"dim128\" : make_cfg(128, 8),\n",
    "}\n",
    "\n",
    "# Pretty-print if you run this file directly\n",
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "    pprint(CONFIGS, width=120, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Scaling Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810256 this is 16 size\n",
      "1219992 this is 24 size\n",
      "1632800 this is 32 size\n",
      "2495280 this is 48 size\n",
      "2927288 this is 56 size\n",
      "3413056 this is 64 size\n",
      "3867336 this is 72 size\n",
      "4404560 this is 80 size\n",
      "5488224 this is 96 size\n",
      "8005760 this is 128 size\n",
      "{'dim16': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 16,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.044721359549995794,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 16205120},\n",
      " 'dim24': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 24,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.03651483716701107,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 24399840},\n",
      " 'dim32': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 32,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.03162277660168379,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 32656000},\n",
      " 'dim48': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 48,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.025819888974716113,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 49905600},\n",
      " 'dim56': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 56,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.023904572186687872,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 58545760},\n",
      " 'dim64': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 64,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.022360679774997897,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 68261120},\n",
      " 'dim72': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 72,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.021081851067789193,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 77346720},\n",
      " 'dim80': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 80,\n",
      "           'num_layers': 5,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.02,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 88091200},\n",
      " 'dim96': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 1,\n",
      "           'max_epochs': 1,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 96,\n",
      "           'num_layers': 6,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.018257418583505537,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 109764480},\n",
      " 'dim128': {'dataset': 'c4_subset',\n",
      "            'lr_schedule': 'cosine',\n",
      "            'warmup_epochs': 1,\n",
      "            'warmup_epochs_frac': 0.1,\n",
      "            'weight_decay': 0.1,\n",
      "            'dropout': 0.0,\n",
      "            'seq_length': 128,\n",
      "            'pos_encoding': 'rotary',\n",
      "            'init_scheme': 'transformer_scaled',\n",
      "            'stride': 64,\n",
      "            'pin_memory': True,\n",
      "            'compile': False,\n",
      "            'prefetch_factor': 8,\n",
      "            'min_epochs': 1,\n",
      "            'max_epochs': 1,\n",
      "            'use_gradient_clipping': True,\n",
      "            'gradient_clip_val': 1.0,\n",
      "            'label_smoothing': 0.0,\n",
      "            'optimizer': 'adamw',\n",
      "            'activation': 'gelu',\n",
      "            'norm_type': 'layer',\n",
      "            'results_folder': 'Former_Experiments_Folder',\n",
      "            'csv_log_interval': 50,\n",
      "            'seed': 789,\n",
      "            'hidden_dim': 128,\n",
      "            'num_layers': 8,\n",
      "            'num_heads': 2,\n",
      "            'learning_rate': 0.015811388300841896,\n",
      "            'batch_size': 32,\n",
      "            'gradient_accumulation_steps': 8,\n",
      "            'train_tokens': 160115200}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# === Literature-backed heuristics ==========================================\n",
    "\n",
    "HEAD_DIM = 64\n",
    "# Vaswani et al., 2017 §3.2: “we set d_k = d_v = d_model / h = 64 for each head.”\n",
    "# https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "# BASE_LR = 6e-4\n",
    "#new rate base on sgd sweep \n",
    "BASE_LR = 10**(-1.5)\n",
    "\n",
    "\n",
    "# Yang et al., 2022 (Tensor Programs V) §4: “optimal LR scales ∝ width^{-½}.”\n",
    "# https://arxiv.org/pdf/2203.03466.pdf\n",
    "\n",
    "CHIN_TOK_RATIO = 20\n",
    "# Hoffmann et al., 2022 (Chinchilla) §3: “compute-optimal: ~20 tokens per parameter.”\n",
    "# https://arxiv.org/pdf/2203.15556.pdf\n",
    "\n",
    "EFF_BS  = 256                      # keep effective batch fixed\n",
    "PHYS_BS = 32                       # Goyal et al., 2017: LR ∝ batch (linear-scaling rule)\n",
    "# https://arxiv.org/pdf/1706.02677.pdf\n",
    "\n",
    "GPT2_VOCAB_SIZE = 50_257\n",
    "\n",
    "# === Scaling rules =========================================================\n",
    "\n",
    "def make_cfg(d_model: int,\n",
    "             vocab: int = GPT2_VOCAB_SIZE):\n",
    "    # Depth grows ≈ 0.06 × width (TinyStories sweep, Eldan & Li 2023, Fig. 5)\n",
    "    # https://arxiv.org/pdf/2305.07759.pdf\n",
    "    n_layers = max(2, round(0.06 * d_model))\n",
    "\n",
    "    # 2–8 heads so each head keeps ≥64-d and avoids the low-rank bottleneck\n",
    "    # Bhojanapalli et al., 2020: “excess heads create a low-rank bottleneck.”\n",
    "    # https://proceedings.mlr.press/v119/bhojanapalli20a/bhojanapalli20a.pdf\n",
    "    # Saratchandran et al., 2025 (Leaner Transformers): “more heads lets you cut depth.”\n",
    "    # https://arxiv.org/pdf/2505.20802.pdf\n",
    "    n_heads  = max(2, min(8, round(d_model / HEAD_DIM)))\n",
    "\n",
    "    # Width-scaled LR (μP theory) + 2025 plateau exponent ≈ 0.22 (Li et al., 2025)\n",
    "    # https://arxiv.org/pdf/2503.04715.pdf\n",
    "    lr_base_dim = 32\n",
    "    lr = BASE_LR * (d_model / lr_base_dim) ** (-0.5)\n",
    "\n",
    "    # Rough GPT-style parameter count (Kaplan et al., 2020 formula)\n",
    "    params = 12 * d_model * d_model * n_layers + vocab * d_model\n",
    "    print(params, f\"this is {d_model} size\")\n",
    "\n",
    "    # Compute-optimal token budget (Chinchilla)\n",
    "    tokens = CHIN_TOK_RATIO * params\n",
    "\n",
    "    return dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = n_heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = PHYS_BS,\n",
    "        gradient_accumulation_steps = EFF_BS // PHYS_BS,\n",
    "        train_tokens = tokens,\n",
    "    )\n",
    "\n",
    "# --- Config grid -----------------------------------------------------------\n",
    "\n",
    "CONFIGS = {\n",
    "    f\"dim{d}\": make_cfg(d) for d in [16, 24, 32, 48, 56, 64, 72, 80, 96, 128]\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "    pprint(CONFIGS, width=100, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Scaling Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated LSTM Config for hidden_size=16 ---\n",
      "  Architecture: 2 layers, 16 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 1.61M\n",
      "  Data: Using 129.0M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=24 ---\n",
      "  Architecture: 2 layers, 24 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 2.42M\n",
      "  Data: Using 193.7M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=32 ---\n",
      "  Architecture: 2 layers, 32 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 3.23M\n",
      "  Data: Using 258.6M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=48 ---\n",
      "  Architecture: 2 layers, 48 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 4.86M\n",
      "  Data: Using 388.9M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=64 ---\n",
      "  Architecture: 2 layers, 64 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 6.50M\n",
      "  Data: Using 519.9M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=96 ---\n",
      "  Architecture: 2 layers, 96 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 9.80M\n",
      "  Data: Using 783.7M characters from dataset\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Generated LSTM Config for hidden_size=128 ---\n",
      "  Architecture: 2 layers, 128 hidden_size\n",
      "  Training: LR=1.00e-03, Dropout=0.10\n",
      "  Estimated Params: 13.13M\n",
      "  Data: Using 1050.2M characters from dataset\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"data_path\": \"../Datasets/wikitext.txt\",\n",
    "    \"tokenizer_path\": \"../gpt2_tokenizer\",\n",
    "    \"max_characters\": 5 * 1e7,  # Maximum number of characters to use from dataset\n",
    "    \"sequence_length\": 128,\n",
    "    \"batch_size\": 32,  # Keep physical batch size small, has no effect on model\n",
    "    \"hidden_size\": 16,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.0,  # dropout zer here to match transformer but may need to adjust for LSTM\n",
    "    \"learning_rate\": 0.001 * math.sqrt(4),  # Scale by sqrt of accumulation steps\n",
    "    \"lr_schedule\": \"cosine\",\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.1,  # parameter usedf for stepLR step decay\n",
    "    \"num_epochs\": 5,\n",
    "    \"train_split\": 0.8,\n",
    "    \"val_split\": 0.1,\n",
    "    \"test_split\": 0.1,\n",
    "    \"device\": \"cuda\",\n",
    "    \"wandb_project\": \"lstm-wikitext\",\n",
    "    \"wandb_offline\": True,\n",
    "    \"print_every\": 100,  # Print loss every N batches\n",
    "    # Gradient clipping settings\n",
    "    \"use_gradient_clipping\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    # NEW: CSV logging settings\n",
    "    \"results_folder\": \"Experiments_Folder\",\n",
    "    \"csv_log_interval\": 100,  # Log every 100 steps\n",
    "    # NEW: Data loading optimization settings\n",
    "    \"num_workers\": \"auto\",  # Will be set automatically based on CPU cores\n",
    "    \"pin_memory\": True,  # Faster GPU memory transfer\n",
    "    \"persistent_workers\": True,  # Keep data loading workers alive between epochs\n",
    "    \"prefetch_factor\": 4,  # Number of batches to prefetch per worker\n",
    "    # NEW: Mixed precision settings\n",
    "    \"use_amp\": False,  # Enable Automatic Mixed Precision\n",
    "    \"amp_opt_level\": \"O1\",  # Not used with native AMP, but kept for reference\n",
    "    # NEW: Gradient accumulation settings\n",
    "    \"gradient_accumulation_steps\": 16,  # For tracking only\n",
    "    # NEW: whether to compile the model (PyTorch 2.0+)\n",
    "    \"use_compile\": False,\n",
    "    \"seed\": 789,\n",
    "    \"optimizer\": \"adamw\",  # NEW: choose from \"adam\", \"adamw\", or \"sgd\"\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"stride\": 64,  # NEW: sliding-window stride to match transformer\n",
    "}\n",
    "\n",
    "\n",
    "def make_lstm_config(hidden_size, base_config=None):\n",
    "    \"\"\"\n",
    "    Generates a scaled LSTM configuration based on hidden dimension size.\n",
    "\n",
    "    This function applies heuristics and best practices for scaling LSTMs,\n",
    "    focusing on increasing width before depth and adjusting learning rate\n",
    "    and data size accordingly.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The target hidden dimension size for the LSTM.\n",
    "        base_config (dict, optional): A base configuration to build upon.\n",
    "                                      If None, uses sensible defaults from CONFIG.\n",
    "\n",
    "    Returns:\n",
    "        dict: A configuration dictionary scaled for the given hidden size.\n",
    "    \"\"\"\n",
    "    # 1. --- Set sensible base config if none provided ---\n",
    "    if base_config is None:\n",
    "        base_config = CONFIG\n",
    "    \n",
    "    config = base_config.copy()\n",
    "    config[\"hidden_size\"] = hidden_size\n",
    "\n",
    "    # 2. --- Scale LSTM architecture (Wider before Deeper) ---\n",
    "    # Depth (num_layers) should scale sub-linearly with width (hidden_size).\n",
    "    if hidden_size <= 128:\n",
    "        config[\"num_layers\"] = 2\n",
    "    elif hidden_size <= 512:\n",
    "        config[\"num_layers\"] = 3\n",
    "    else:\n",
    "        # It's rare for LSTMs to benefit from more than 4 layers\n",
    "        config[\"num_layers\"] = 4\n",
    "\n",
    "    # 3. --- Scale Learning Rate ---\n",
    "    # Use inverse square root scaling relative to a baseline.\n",
    "    base_lr = 1e-3\n",
    "    base_hidden = 128\n",
    "    # LR decreases as model size increases\n",
    "   \n",
    "    config['learning_rate'] = base_lr\n",
    "\n",
    "    # 4. --- Scale Dropout for Regularization ---\n",
    "    # Larger models can handle more dropout.\n",
    "    if hidden_size <= 256:\n",
    "        config[\"dropout\"] = 0.1\n",
    "    else:\n",
    "        config[\"dropout\"] = 0.2\n",
    "\n",
    "    # 5. --- Scale Data Amount (Chinchilla-style) ---\n",
    "    # First, estimate model parameters.\n",
    "    vocab_size = 50257  # GPT-2\n",
    "    # Params for one LSTM layer: 4 * (h*h (input-hidden) + h*h (hidden-hidden) + 2*h (biases))\n",
    "    # Simplified: 4 * (2*h^2) = 8*h^2\n",
    "    lstm_params = config[\"num_layers\"] * (8 * hidden_size**2)\n",
    "    embedding_params = vocab_size * hidden_size\n",
    "    output_params = hidden_size * vocab_size\n",
    "    total_params = lstm_params + embedding_params + output_params\n",
    "\n",
    "    # Chinchilla's rule: ~20 tokens per parameter\n",
    "    target_tokens = 20 * total_params\n",
    "    # Assume ~4 characters per token for English text\n",
    "    chars_per_token = 4\n",
    "    config[\"max_characters\"] = int(target_tokens * chars_per_token)\n",
    "\n",
    "    print(f\"\\n--- Generated LSTM Config for hidden_size={hidden_size} ---\")\n",
    "    print(f\"  Architecture: {config['num_layers']} layers, {config['hidden_size']} hidden_size\")\n",
    "    print(f\"  Training: LR={config['learning_rate']:.2e}, Dropout={config['dropout']:.2f}\")\n",
    "    print(f\"  Estimated Params: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"  Data: Using {config['max_characters'] / 1e6:.1f}M characters from dataset\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# Example of how to use the generator to create an experiment suite\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define the hidden sizes you want to test\n",
    "    lstm_hidden_sizes = [16, 24, 32, 48, 64, 96, 128]\n",
    "\n",
    "    # Create a dictionary of configuration objects\n",
    "    LSTM_EXPERIMENTS = {\n",
    "        f\"lstm_h{size}\": make_lstm_config(size) for size in lstm_hidden_sizes\n",
    "    }\n",
    "\n",
    "    # Now you can iterate through these configs to run your experiments\n",
    "    # For example:\n",
    "    # for exp_name, exp_config in LSTM_EXPERIMENTS.items():\n",
    "    #     print(f\"\\nRunning experiment: {exp_name}\")\n",
    "    #     # Here you would call your training function with exp_config\n",
    "    #     # train(config=exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
