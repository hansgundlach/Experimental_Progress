{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset': 'c4_subset', 'batch_size': 1, 'learning_rate': 0.001, 'min_lr': 1e-05, 'lr_schedule': 'cosine', 'warmup_epochs': 1, 'warmup_epochs_frac': 0.1, 'weight_decay': 0.1, 'hidden_dim': 256, 'num_layers': 16, 'num_heads': 16, 'dropout': 0.0, 'seq_length': 128, 'wikitext_limit': 50000000, 'pos_encoding': 'rotary', 'init_scheme': 'transformer_scaled', 'stride': 64, 'pin_memory': True, 'compile': False, 'prefetch_factor': 8, 'min_epochs': 2, 'max_epochs': 218454, 'use_gradient_clipping': True, 'gradient_clip_val': 2.0, 'label_smoothing': 0.0, 'gradient_accumulation_steps': 16, 'optimizer': 'adamw', 'activation': 'gelu', 'norm_type': 'layer', 'results_folder': 'Former_Experiments_Folder', 'csv_log_interval': 50, 'seed': 789, 'target_tokens': 251658240}]\n"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "\n",
    "base_config = {\n",
    "        \"dataset\": \"c4_subset\",\n",
    "        \"batch_size\": 32,  # physical batch size 256\n",
    "        \"learning_rate\": 0.001 * math.sqrt(4),\n",
    "        \"min_lr\": 1e-5,\n",
    "        \"lr_schedule\": \"cosine\",\n",
    "        \"warmup_epochs\": 1,\n",
    "        \"warmup_epochs_frac\": 0.1,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"hidden_dim\": 64,  # Base hidden dimension\n",
    "        \"num_layers\": 4,  # Base number of layers\n",
    "        \"num_heads\": 4,\n",
    "        \"dropout\": 0.0,\n",
    "        \"seq_length\": 128,\n",
    "        \"wikitext_limit\": 5 * 10**7,\n",
    "        \"pos_encoding\": \"rotary\",\n",
    "        \"init_scheme\": \"transformer_scaled\",\n",
    "        \"stride\": 64,\n",
    "        \"pin_memory\": True,\n",
    "        \"compile\": False,\n",
    "        \"prefetch_factor\": 8,\n",
    "        \"min_epochs\": 2,\n",
    "        \"max_epochs\": 2,\n",
    "        \"use_gradient_clipping\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"label_smoothing\": 0.0,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"optimizer\": \"adamw\",\n",
    "        \"activation\": \"gelu\",\n",
    "        \"norm_type\": \"layer\",\n",
    "        \"results_folder\": \"Former_Experiments_Folder\",\n",
    "        \"csv_log_interval\": 50,\n",
    "        \"seed\": 789,\n",
    "    }\n",
    "\n",
    "def chinchilla_scale(base_cfg, hidden_dims):\n",
    "    \"\"\"\n",
    "    Return a list of configs that satisfy:\n",
    "      • tokens ≈ 20 × parameters\n",
    "      • per-step compute budget unchanged vs. baseline\n",
    "      • depth/width ratio fixed (layers ∝ hidden_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def param_count(d, L):\n",
    "        # crude but width-dominant: 12·L·d²  (ignores embeddings/out-proj)\n",
    "        return 12 * L * d**2\n",
    "\n",
    "    base_d = base_cfg[\"hidden_dim\"]\n",
    "    base_L = base_cfg[\"num_layers\"]\n",
    "    base_bsz = base_cfg[\"batch_size\"]\n",
    "    base_lr = base_cfg[\"learning_rate\"]\n",
    "    base_clip = base_cfg[\"gradient_clip_val\"]\n",
    "    seq_len = base_cfg[\"seq_length\"]\n",
    "\n",
    "    out = []\n",
    "    for d in hidden_dims:\n",
    "        width_scale = d / base_d\n",
    "\n",
    "        # 1) Depth: keep L ∝ d   (so aspect-ratio is preserved)\n",
    "        L = max(1, int(round(base_L * width_scale)))\n",
    "\n",
    "        # 2) Keep per-step FLOPs ≈ const ⇒ batch ∝ 1 / (width² · depth/base_depth)\n",
    "        flops_scale = (width_scale**2) * (L / base_L)\n",
    "        bsz = max(1, int(round(base_bsz / flops_scale)))\n",
    "\n",
    "        # 3) LR & grad-clip heuristics\n",
    "        lr = base_lr * (base_d / d) ** 0.5\n",
    "        clip = base_clip * math.sqrt(width_scale)\n",
    "\n",
    "        # 4) Chinchilla target tokens  (≈ 20 × parameters)\n",
    "        params = param_count(d, L)\n",
    "        tgt_tok = int(20 * params)\n",
    "\n",
    "        # 5) Convert token target into epochs\n",
    "        tokens_per_step = bsz * seq_len\n",
    "        est_steps = math.ceil(tgt_tok / tokens_per_step)\n",
    "        max_epochs = math.ceil(\n",
    "            est_steps / (len(base_cfg.get(\"dataset\", [])) or 1)\n",
    "        )  # adjust as needed\n",
    "\n",
    "        cfg = copy.deepcopy(base_cfg)\n",
    "        cfg.update(\n",
    "            {\n",
    "                \"hidden_dim\": d,\n",
    "                \"num_layers\": L,\n",
    "                \"num_heads\": max(1, d // 16),\n",
    "                \"batch_size\": bsz,\n",
    "                \"learning_rate\": lr,\n",
    "                \"gradient_clip_val\": clip,\n",
    "                \"target_tokens\": tgt_tok,\n",
    "                \"max_epochs\": max(max_epochs, cfg.get(\"min_epochs\", 1)),\n",
    "            }\n",
    "        )\n",
    "        out.append(cfg)\n",
    "    return out\n",
    "\n",
    "\n",
    "print(chinchilla_scale(base_config, [256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overrides:\n",
      "{'hidden_dim': 16, 'num_layers': 2, 'num_heads': 1, 'learning_rate': 0.001, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 16205120}\n",
      "810256 for  16 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 32, 'num_layers': 3, 'num_heads': 2, 'learning_rate': 0.0014142135623730952, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 32901760}\n",
      "1645088 for  32 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 64, 'num_layers': 4, 'num_heads': 4, 'learning_rate': 0.002, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 68261120}\n",
      "3413056 for  64 d model\n",
      "Overrides:\n",
      "{'hidden_dim': 128, 'num_layers': 8, 'num_heads': 8, 'learning_rate': 0.0028284271247461905, 'batch_size': 32, 'gradient_accumulation_steps': 8, 'train_tokens': 160115200}\n",
      "8005760 for  128 d model\n",
      "{'dim16': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 2,\n",
      "           'max_epochs': 2,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 16,\n",
      "           'num_layers': 2,\n",
      "           'num_heads': 1,\n",
      "           'learning_rate': 0.001,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 16205120},\n",
      " 'dim32': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 2,\n",
      "           'max_epochs': 2,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 32,\n",
      "           'num_layers': 3,\n",
      "           'num_heads': 2,\n",
      "           'learning_rate': 0.0014142135623730952,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 32901760},\n",
      " 'dim64': {'dataset': 'c4_subset',\n",
      "           'lr_schedule': 'cosine',\n",
      "           'warmup_epochs': 1,\n",
      "           'warmup_epochs_frac': 0.1,\n",
      "           'weight_decay': 0.1,\n",
      "           'dropout': 0.0,\n",
      "           'seq_length': 128,\n",
      "           'pos_encoding': 'rotary',\n",
      "           'init_scheme': 'transformer_scaled',\n",
      "           'stride': 64,\n",
      "           'pin_memory': True,\n",
      "           'compile': False,\n",
      "           'prefetch_factor': 8,\n",
      "           'min_epochs': 2,\n",
      "           'max_epochs': 2,\n",
      "           'use_gradient_clipping': True,\n",
      "           'gradient_clip_val': 1.0,\n",
      "           'label_smoothing': 0.0,\n",
      "           'optimizer': 'adamw',\n",
      "           'activation': 'gelu',\n",
      "           'norm_type': 'layer',\n",
      "           'results_folder': 'Former_Experiments_Folder',\n",
      "           'csv_log_interval': 50,\n",
      "           'seed': 789,\n",
      "           'hidden_dim': 64,\n",
      "           'num_layers': 4,\n",
      "           'num_heads': 4,\n",
      "           'learning_rate': 0.002,\n",
      "           'batch_size': 32,\n",
      "           'gradient_accumulation_steps': 8,\n",
      "           'train_tokens': 68261120},\n",
      " 'dim128': {'dataset': 'c4_subset',\n",
      "            'lr_schedule': 'cosine',\n",
      "            'warmup_epochs': 1,\n",
      "            'warmup_epochs_frac': 0.1,\n",
      "            'weight_decay': 0.1,\n",
      "            'dropout': 0.0,\n",
      "            'seq_length': 128,\n",
      "            'pos_encoding': 'rotary',\n",
      "            'init_scheme': 'transformer_scaled',\n",
      "            'stride': 64,\n",
      "            'pin_memory': True,\n",
      "            'compile': False,\n",
      "            'prefetch_factor': 8,\n",
      "            'min_epochs': 2,\n",
      "            'max_epochs': 2,\n",
      "            'use_gradient_clipping': True,\n",
      "            'gradient_clip_val': 1.0,\n",
      "            'label_smoothing': 0.0,\n",
      "            'optimizer': 'adamw',\n",
      "            'activation': 'gelu',\n",
      "            'norm_type': 'layer',\n",
      "            'results_folder': 'Former_Experiments_Folder',\n",
      "            'csv_log_interval': 50,\n",
      "            'seed': 789,\n",
      "            'hidden_dim': 128,\n",
      "            'num_layers': 8,\n",
      "            'num_heads': 8,\n",
      "            'learning_rate': 0.0028284271247461905,\n",
      "            'batch_size': 32,\n",
      "            'gradient_accumulation_steps': 8,\n",
      "            'train_tokens': 160115200}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# ---------- common settings ----------\n",
    "COMMON = dict(\n",
    "    dataset             = \"c4_subset\",\n",
    "    lr_schedule         = \"cosine\",\n",
    "    warmup_epochs       = 1,\n",
    "    warmup_epochs_frac  = 0.10,\n",
    "    weight_decay        = 0.10,\n",
    "    dropout             = 0.0,          # bump to 0.1-0.2 for >100 M tokens if needed\n",
    "    seq_length          = 128,\n",
    "    pos_encoding        = \"rotary\",\n",
    "    init_scheme         = \"transformer_scaled\",\n",
    "    stride              = 64,\n",
    "    pin_memory          = True,\n",
    "    compile             = False,\n",
    "    prefetch_factor     = 8,\n",
    "    min_epochs          = 1,\n",
    "    max_epochs          = 1,\n",
    "    use_gradient_clipping = True,\n",
    "    gradient_clip_val   = 1.0,\n",
    "    label_smoothing     = 0.0,\n",
    "    optimizer           = \"adamw\",\n",
    "    activation          = \"gelu\",\n",
    "    norm_type           = \"layer\",\n",
    "    results_folder      = \"Former_Experiments_Folder\",\n",
    "    csv_log_interval    = 50,\n",
    "    seed                = 789,\n",
    ")\n",
    "\n",
    "GPT2_VOCAB_SIZE = 50257\n",
    "\n",
    "def make_cfg(d_model, n_layers, vocab:int = GPT2_VOCAB_SIZE):\n",
    "    heads = max(1, d_model // 16)\n",
    "    lr    = 0.001 * math.sqrt(d_model / 16)\n",
    "    params = 12 * d_model * d_model * n_layers + vocab * d_model    # rough GPT-style count\n",
    "    tokens = 20 * params                           # Chin. optimal compute\n",
    "    eff_bs = 256                                   # keep effective batch ~constant\n",
    "    phys_bs = 32     \n",
    "    \n",
    "    print(\"Overrides:\")     \n",
    "    print(dict(\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    ))        \n",
    "    print(params, \"for \", d_model, \"d model\")                 # fits typical 40 GB A100 w/ acc-16\n",
    "\n",
    "    return dict(\n",
    "        COMMON,\n",
    "        hidden_dim  = d_model,\n",
    "        num_layers  = n_layers,\n",
    "        num_heads   = heads,\n",
    "        learning_rate = lr,\n",
    "        batch_size  = phys_bs,\n",
    "        gradient_accumulation_steps = eff_bs // phys_bs,\n",
    "        train_tokens = tokens,         # ← 1 epoch budget\n",
    "    )\n",
    "\n",
    "\n",
    "#choose layer depth so that it is roughly proportional to the hidden dimension cubed\n",
    "CONFIGS = {\n",
    "    \"dim16\"  : make_cfg(16,  2),\n",
    "    \"dim32\"  : make_cfg(32,  3),\n",
    "    \"dim64\"  : make_cfg(64,  4),   # original width/depth\n",
    "    \"dim128\" : make_cfg(128, 8),\n",
    "}\n",
    "\n",
    "# Pretty-print if you run this file directly\n",
    "if __name__ == \"__main__\":\n",
    "    from pprint import pprint\n",
    "    pprint(CONFIGS, width=120, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
